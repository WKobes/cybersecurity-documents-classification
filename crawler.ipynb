{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # To get the path of this file\n",
    "import scrapy # Library for the crawler\n",
    "import json # To output our data in JSON format\n",
    "import logging\n",
    "import threading # For locks\n",
    "import PyPDF2 # To parse PDF files when a HTML version is not available\n",
    "import requests # Easier to get PDF files\n",
    "\n",
    "# Scrapy specific imports\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "# Settings for notebook & scrapy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Make a lock for counter value\n",
    "lock = threading.RLock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_KEYS = [ # The search terms we are using to retrieve information\n",
    "    'cyber%20security',\n",
    "#     'cybersecurity',\n",
    "#     'cybercrime',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonWriterPipeline(object):\n",
    "    \"\"\"\n",
    "    This pipeline will output all data in one JSON file, line separated.\n",
    "    \"\"\"\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        \"\"\"\n",
    "        When starting this pipeline, the file needs to be openend.\n",
    "        \"\"\"\n",
    "        self.file = open('euresult.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        \"\"\"\n",
    "        When the crawler is done, the file needs to be closed.\n",
    "        \"\"\"\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"\n",
    "        Every item that is processed is written to a new line in the file.\n",
    "        \"\"\"\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "    \n",
    "\n",
    "class JsonSeperateFileWriterPipeline(object):\n",
    "    \"\"\"\n",
    "    This pipeline will output all data in seperate JSON files, using the ID as file name, storing in the directory 'files'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def open_spider(self, spider):\n",
    "        \"\"\"\n",
    "        We do not have to do anything when starting this pipeline\n",
    "        \"\"\"\n",
    "        logging.info(msg=\"Starting JsonSeperateFileWriterPipeline\")\n",
    "        \n",
    "    def close_spider(self, spider):\n",
    "        \"\"\"\n",
    "        We print the statistics for this run when the spider quits\n",
    "        \"\"\"\n",
    "        logging.info(\"Finished! We processed \" + str(spider.item_counter) + \" files.\")\n",
    "        logging.info(\"Closing JsonSeperateFileWriterPipeline\")\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        \"\"\"\n",
    "        Every item that is processed is written to a seperate json file.\n",
    "        \"\"\"\n",
    "        # TODO: check if this approach works on Windows\n",
    "        tmp = open(os.path.abspath(os.curdir) + '/files/{}.json'.format(item['local_id'].replace('/', '')), 'w')\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        \n",
    "        tmp.write(line)\n",
    "        tmp.close()\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EucybsecSpider(CrawlSpider):\n",
    "    \"\"\"\n",
    "    The spider designed for the European Union CELEX website.\n",
    "    \"\"\"\n",
    "    \n",
    "    name = 'eucybsec'\n",
    "    allowed_domains = ['eur-lex.europa.eu'] # URLs outside these domains are not followed\n",
    "    start_urls = [ # TODO: Soft-code search terms\n",
    "        'https://eur-lex.europa.eu/search.html?text=cyber%20security&scope=EURLEX&type=quick&lang=en'\n",
    "    ]\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL': logging.INFO,\n",
    "        'ITEM_PIPELINES': {\n",
    "#             '__main__.JsonWriterPipeline': 1,\n",
    "            '__main__.JsonSeperateFileWriterPipeline': 1,\n",
    "        }, \n",
    "    }\n",
    "    \n",
    "    rules = (\n",
    "        Rule(\n",
    "            LinkExtractor(\n",
    "                allow=(),\n",
    "                restrict_xpaths=('(//a[@title=\"Next Page\"])[1]',), # This will extract the URL for the next page, if existent\n",
    "            ),\n",
    "            callback='parse_item',\n",
    "            follow=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Non scrapy variables\n",
    "    item_counter = 0\n",
    "    MAIN_DOMAIN = \"https://eur-lex.europa.eu\"\n",
    "\n",
    "    def parse_document_page(self, response):\n",
    "        \"\"\"\n",
    "        Yield the values for one specific document\n",
    "        \"\"\"\n",
    "        \n",
    "        # The neatest way to get the document date, but not always available\n",
    "        document_date = response.xpath('//meta[@property=\"eli:date_document\"]/@content').extract_first()\n",
    "        \n",
    "        # The other way to get the document date.\n",
    "        if not document_date:\n",
    "            document_date = response.xpath('//p[@class=\"hd-date\"]/text()').extract_first()\n",
    "            if document_date:\n",
    "                document_date = document_date.strip()    \n",
    "        \n",
    "        # Compress the content for efficiency\n",
    "        # TODO: See #5\n",
    "        content_elements = response.xpath('//div[@class=\"tabContent\"]//text()').extract()\n",
    "        content = ' '.join(content_elements)\n",
    "        \n",
    "        # The HTML version is not available, so we are going to extract the PDF\n",
    "        if content == \"\":\n",
    "            \n",
    "            content = self.pdf_retriever(response)\n",
    "            \n",
    "        \n",
    "        # Minimize the content for efficiency\n",
    "        content = ' '.join(content.split()).lower()\n",
    "        \n",
    "        with lock:\n",
    "            self.item_counter += 1\n",
    "            this_item = self.item_counter\n",
    "        \n",
    "        yield {\n",
    "            'title': response.xpath(\"//p[@id='translatedTitle']/text()\").extract_first(),\n",
    "            'local_id': response.xpath('//*[contains(@class,\"DocumentTitle\")]/text()').extract_first().split()[1],\n",
    "            'order_id': this_item,\n",
    "            'document_type': response.xpath('//meta[@property=\"eli:type_document\"]/@resource').extract_first(),\n",
    "            'publication_date': response.xpath('//meta[@property=\"eli:date_publication\"]/@content').extract_first(),\n",
    "            'document_date': document_date,\n",
    "            'direct_url': response.url,\n",
    "            'content': content,\n",
    "        }\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        \"\"\"\n",
    "        Parsing a page with search results, calling for the next page in the end.\n",
    "        \"\"\"\n",
    "        \n",
    "#         print('Processing.. ' + response.url)\n",
    "        item_links = response.xpath(\"//div[contains(@class,'SearchResult')]/h2/a/@href\").extract()\n",
    "            \n",
    "        item_links = [self.MAIN_DOMAIN + rel_path[1:] for rel_path in item_links]\n",
    "\n",
    "        for a in item_links:\n",
    "            yield scrapy.Request(a, callback=self.parse_document_page)\n",
    "            \n",
    "    \n",
    "    def parse_start_url(self, response):\n",
    "        return self.parse_item(response)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pdf_retriever(self, response):\n",
    "        # Get the relative URL of the PDF file\n",
    "        pdf_href = response.xpath(\"//a[@id='format_language_table_PDF_EN']/@href\").extract_first()\n",
    "        \n",
    "        if not pdf_href:\n",
    "            logging.info(\"PDF URL not located.\")\n",
    "            return \"\"\n",
    "\n",
    "        # Make the URL absolute\n",
    "        pdf_href = self.MAIN_DOMAIN + pdf_href.split(\"..\")[-1]\n",
    "        \n",
    "        logging.info(pdf_href)\n",
    "        \n",
    "        try:\n",
    "            remoteFile = requests.get(pdf_href)\n",
    "        except:\n",
    "            logging.info(\"Could not get PDF.\")\n",
    "            return \"\"\n",
    "        \n",
    "        # Temporary store the PDF file locally\n",
    "        with open(\"tmp.pdf\", 'wb') as my_data:\n",
    "            my_data.write(remoteFile.content)\n",
    "\n",
    "        open_pdf_file = open(\"tmp.pdf\", 'rb')\n",
    "        \n",
    "        try:\n",
    "            pdfReader = PyPDF2.PdfFileReader(open_pdf_file)\n",
    "        except Exception as e:\n",
    "            logging.info(\"Could not read PDF. Error:\")\n",
    "            logging.info(e)\n",
    "            return \"\"\n",
    "\n",
    "        content = \"\"\n",
    "        \n",
    "        # append all the pages to the content\n",
    "        for page_number in range(pdfReader.numPages):\n",
    "            content += pdfReader.getPage(page_number).extractText()\n",
    "        \n",
    "        return content\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crawler():\n",
    "    \"\"\"\n",
    "    Start the crawler\n",
    "    \"\"\"\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "    })\n",
    "\n",
    "    process.crawl(EucybsecSpider)\n",
    "    process.start()\n",
    "    \n",
    "# TODO: Figure out how to 'restart' without having to fully restart the kernel.\n",
    "run_crawler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
